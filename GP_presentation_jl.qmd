---
title: "Gaussian Processes"
author:
  - name: Francisco Cardozo
    affiliation: University of Miami
    department: Public Health
format: revealjs
jupyter: julia-1.8
---

# Agenda {background-color=#4deeea}

:::: {.columns}

::: {.column width="45%"}

* What are Gaussian Processes (GP)?

* Simple problem: Linear regression and Polynomial regression.

* Problem with polynomials and alternatives Splines/loess.

:::

::: {.column width="10%"}
<!-- empty column to create gap -->
:::

::: {.column width="45%"}

* Introduction to Kernels and their types.

* Exploring Gaussian Processes: Estimation, Predictions, and Inference.

* Practical Example.
  
:::

::::


# What is Gaussian Processes (GP) {background-color=#f000ff}

## GP

A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. This stochastic process generalizes the notion of a Gaussian distribution to infinite-dimensional random vectors.

In simpler terms, a Gaussian Process is a tool for making predictions when we have some information, but not complete information. For instance, if you have a number of points on a graph and you want to draw a line that smoothly connects them, Gaussian Processes can assist in achieving this through mathematical computation!


```{julia}
using Pkg
#Pkg.add("DataFrames")
#Pkg.add("GLM")
#Pkg.add("StatsModels")
#Pkg.add("Distributions")
```

```{julia}
using DataFrames, GLM, Plots
```

## GP

* GPR is a non-parametric model, which means that it does not make any assumptions about the underlying distribution of the data. 

* Can make predictions incorporation prior knowledge (kernels) and provide uncertainty measures over prediction. 


# Simulated data example  {background-color=#74ee15}

## Simulated data

Let's create some data using the Sine function and introduce some noise to this data.

```{julia}
#| echo: true
#| output: false

x = range(0, 10, length=30)
y = sin.(x)
y_noisy = @. sin(x) + 0.3*randn()

```

## Plot generated data

We can visualize our generated data with the following code:

```{julia}

data = DataFrame(x=x,y=y_noisy)

plot(data.x, data.y, seriestype=:scatter, label="data")

```
# Linear Regression {background-color=#ffe700}

## Estimate the linear regression

We'll use a basic Ordinary Least Squares (OLS) regression model to estimate the relationship between our variables.

```{julia}
#| messages: false
#| warning: false
ols = lm(@formula(y ~ x), data)
pred = DataFrame(x = x)
pred.y = predict(ols, pred)
ols
```

## Plot the linear regression

We can also plot our estimated model:

```{julia}

plot!(data.x, pred.y, seriestype=:line, label="Linear")

```

# Polynomial Regression {background-color=#74ee15} 

## Estimating the Polynomial Regression

Polynomial regression extends simple linear regression by allowing for more flexible relationships between the x and y. It does this by adding higher-order terms (squares, cubes, etc.) of the predictor variables.

```{julia}
ols_p = lm(@formula(y ~ x + x^2), data)
pred.y_p = predict(ols_p, pred)
ols_p
```

## Plotting the Polynomial Regression

```{julia}
plot!(data.x, pred.y_p, seriestype=:line, label="Ploy")
```

## Estimating the Polynomial Regression Part II

```{julia}
ols_p = lm(@formula(y ~ x + x^2 + x^3 + x^4), data)
pred.y_p = predict(ols_p, pred)
ols_p
```

## Plotting the Polynomial Regression Part. II

```{julia}
plot!(data.x, pred.y_p, seriestype=:line, label="Ploy")
```

## Estimating the Polynomial Regression Part III

```{julia}
ols_mp = lm(@formula(y ~ x + x^2 + x^3 + x^4 + x^5 + x^6 + x^7), data)
pred.y_mp = predict(ols_mp, pred)
ols_mp
```

## Plotting the Polynomial Regression Part III

```{julia}
plot!(data.x, pred.y_mp, seriestype=:line, label="Ploy 7")
```

# Limitations of this approach

## What are the Weakness of this Approach

Very easy, but, what if we have more variables?

There are some limitations to using polynomial regression, especially when dealing with multiple variables:

* Potential for overfitting. 
* Lack of smoothness. 
* Difficulty with predictions.
* Intractability when dealing with multiple variables.

## The Solution

::: {.r-fit-text}

To overcome the limitations of polynomial regression, we can use other techniques like Splines and LOESS:

**Splines:** The idea is to fit different low-degree polynomial functions over different parts of the data. This can provide a more flexible fit than a single polynomial over the entire range of the data.

**LOESS (LOcally Estimated Scatterplot Smoothing) or LOWESS (LOcally WEighted Scatterplot Smoothing):** for each point in the dataset, a subset of neighboring points is selected, and a polynomial (usually linear or quadratic) is fit to this subset of data. The point is then estimated using this local fit. The process is repeated for each point in the dataset to generate a smooth curve that captures the local trends in the data.

:::

# Kernels  {background-color=#001eff}  

## What is a Kernel?

::: {.r-fit-text}

* Kernels are key to understanding and implementing Gaussian Processes.

* The core idea of Kernel methods is to map the input data into a higher-dimensional space where it is easier to apply linear methods

> A kernel is a function that computes a measure of similarity or a relationship between two data points or two sets of data. The output of a kernel function is a scalar that represents the inner product of two data points in a high dimensional feature space. This allows complex relationships to be modeled in this higher-dimensional space.

**The kernel function is used to construct the covariance matrix of the Gaussian process.**

:::

## What Kernels you Should Know?

**Linear Kernel:** The linear kernel is the simplest kernel function. It is given by the inner product <x,y> + c for a constant c and data vectors x and y (same than linear regression).

**Polynomial Kernel:** The polynomial kernel is a more general type of kernel that can model nonlinear decision boundaries. It is given by (scale*<x,y> + c)^d for a constant c, a scale parameter scale, a degree d, and data vectors x and y (same than polynomial regression).

## What Kernels you Should Know?

**Radial Basis Function (RBF):** It is given by exp(-gamma*||x-y||^2) (like a normal distribution) for a constant gamma (two tunning parameters) and data vectors x and y. The RBF kernel maps data into an infinite-dimensional space, making it a powerful tool for many machine learning algorithms.

**Sigmoid Kernel, Laplacian Kernel, Spectral kernel, Mahalonibus kernel,...**

* Must be `positive definitive`
* You can create your own kernel (i.e. linear*periodic)

## How to use Kernels to Model the Simulated Data
::: {.r-fit-text}

* Use the package is call `GaussianProcesses`
* Specify the mean and the kernel 

*Mean*: Zero mean function  
*Kernel*: Squared exponential kernel (note that hyperparameters are on the log scale)  
*Noise*: log standard deviation of observation noise (this is optional)

:::

```{julia}
#| echo: true
#| output: false

using GaussianProcesses

mZero = MeanZero()     
kern = SE(0.0,0.0)
logObsNoise = -1.0  

```

## Estimate a Gaussian Process

```{julia}
#| echo: true

gp = GP(x, y_noisy,mZero,kern,logObsNoise) 

```

## Plot Gaussian Processes

```{julia}
plot(gp)
```

## Advantages of GP

* GP is a non-parametric model, which means that it does not make any assumptions about the underlying distribution of the data. This makes GPR a flexible and powerful tool for a wide variety of applications.

* GPR is able to model nonlinear relationships between input features and output variables.

* GPR is able to provide uncertainty estimates for its predictions.

## Disadvantages of GP

* GPR can be computationally expensive, especially for large datasets.

* GPR can be sensitive to the choice of kernel function.

* GPR can be difficult to interpret.

## GP in Prevention Science

**Identifying risk factors.** Gaussian Processes (GPs) can be used to identify risk factors for negative outcomes, especially those that are hidden behind complex relationships. GPs can model these complex relationships, potentially revealing risk factors that simpler models might miss. By identifying these risk factors, prevention scientists can develop targeted interventions to mitigate the risk of negative outcomes. In addition, the probabilistic nature of GPs provides a measure of uncertainty around the identified risk factors, which is valuable information for decision-making.

## GP in Prevention Science

**Evaluating the effectiveness of prevention programs.** Due to their flexibility, GPs can be used to evaluate the efficacy and effectiveness of prevention programs. They can account for complex and non-linear relationships between variables, which can lead to more accurate estimates of program effects. Furthermore, the ability to provide a measure of uncertainty around these estimates can be crucial for understanding the range of possible effects and for making informed decisions about the implementation of these programs.

## GP in Prevention Science

**Personalizing prevention interventions.** GPs can be instrumental in personalizing prevention interventions. By taking into account individual risk factors and their complex interactions, GPs can aid in the development of interventions tailored to the specific needs of each individual. This could lead to more effective interventions, as they are designed with the intricacies of individual circumstances in mind.

## GP in Prevention Science

**Understanding in cases of low information.** GPs are particularly useful when data is scarce. Even with a small amount of data, a GP can provide a reasonable estimate of the underlying function along with a measure of uncertainty. This means that they can provide useful insights even in low-information situations, where other models might struggle. This can be particularly valuable in prevention science, where often the goal is to act quickly to prevent negative outcomes, and waiting for more data may not be feasible.

# How GP Works? {background-color=#74ee15}  

## Let's Go Beyond

```{julia}
#| echo: true
#| output: false

using Distributions

# Set the mean and covariance matrix
μ = [0.0, 0.0]
Σ = [1.0 0.8; 0.8 1.0]

# Create a multivariate normal distribution with the desired parameters
d = MvNormal(μ, Σ)

# Generate 100 samples from the distribution
n = 100
x = rand(d, n)'

# Print the mean and covariance of the generated samples
println("Mean: ", mean(x))
println("Covariance: ", cov(x))


```

## Plot the Data

```{julia}
#| echo: true
#| output: false

df = DataFrame(x, [:var1, :var2])
plot(df.var1, df.var2, seriestype=:scatter, label="data")
```


## Simulate Multivariate Normal Data Set 

*Multivariate normal distribution:* A multivariate normal distribution is a probability distribution that describes the joint distribution of a set of variables.

**Joint and conditional probability:** Joint probability is the probability that two or more events will occur simultaneously. Conditional probability is the probability that an event will occur given that another event has already occurred.

## Simulate Multivariate Normal Data Set 

```{julia}
#| echo: true
#| output: false

μ = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Σ = [1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.02; 0.8 1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1; 0.7 0.8 1.0 0.8 0.7 0.6 0.5 0.4 0.3 0.2; 0.6 0.7 0.8 1.0 0.8 0.7 0.6 0.5 0.4 0.3; 0.5 0.6 0.7 0.8 1.0 0.8 0.7 0.6 0.5 0.4; 0.4 0.5 0.6 0.7 0.8 1.0 0.8 0.7 0.6 0.5; 0.3 0.4 0.5 0.6 0.7 0.8 1.0 0.8 0.7 0.6; 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0 0.8 0.7; 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0 0.8; 0.02 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0]

# Create a multivariate normal distribution with the desired parameters
d = MvNormal(μ, Σ)

# Generate 100 samples from the distribution
n = 100
x = rand(d, n)'
```

```{julia}
data_f = DataFrame(x, [:var1, :var2, :var3, :var4, :var5, :var6, :var7, :var8, :var9, :var10])

using CSV
CSV.write("data.csv", data_f)

```

## R Quarto Here

Flip to presentation in R. 

## Predict and Inference.

```{julia}
#| echo: true
#| output: false

using GaussianProcesses

mZero = MeanZero()     #Zero mean function
kern = SE(0.0,0.0)     #Sqaured exponential kernel (note that hyperparameters are on the log scale)

logObsNoise = -1.0   # log standard deviation of observation noise (this is optional)
gp = GP(data_f.var1,data_f.var2,mZero,kern,logObsNoise) 

```

## After Fiting we can Predict the Mean and Covariance

```{julia}
#| echo: true
#| output: false
#| 
μ, σ² = predict_y(gp,range(0,stop=2π,length=100));
μ
σ²
```

## Plot GP

```{julia}
#| echo: true
#| output: false

plot(gp; xlabel="var1", ylabel="var2", title="Gaussian process", legend=false, fmt=:png) 

```


